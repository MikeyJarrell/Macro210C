# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 1000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance sqrt(30000)
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 1000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
#clear all
rm.list <- ls()
# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 10000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance sqrt(30000)
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 1000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
summary(first_stage)
#clear all
rm.list <- ls()
# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 10000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance 30000
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 5000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
summary(first_stage)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
#clear all
rm.list <- ls()
# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 100000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance 30000
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 5000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
summary(first_stage)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
#clear all
rm.list <- ls()
# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 1000000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance 30000
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 5000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
summary(first_stage)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
#clear all
rm.list <- ls()
# Load necessary library
library(MASS)
# Set seed for reproducibility
set.seed(123)
# Number of observations
n <- 1000000
# Generate IQ, normally distributed with mean 0 and variance 1
IQ <- rnorm(n, mean = 0, sd = 1)
# Generate Male, binary, randomly assigned
Male <- rbinom(n, size = 1, prob = 0.5)
# Generate Jan, binary, randomly assigned
Jan <- rbinom(n, size = 1, prob = 1/12)
# Error term u for Education equation, normally distributed with mean 0 and variance 4
u <- rnorm(n, mean = 0, sd = 2)
# Education equation
Education <- 8 + 4 * IQ + 2 * Jan + u
# Error term v for Promotions equation, normally distributed with mean 0 and variance 2
v <- rnorm(n, mean = 0, sd = sqrt(2))
# Promotions equation
Promotions <- 3 * Male + v
# Error term e for Wages equation, normally distributed with mean 0 and variance 30000
e <- rnorm(n, mean = 0, sd = sqrt(30000))
# Wages equation
Wages <- 50000 + 5000 * Education + 50000 * Male + 20000 * Promotions + 30000 * IQ + e
# Combine variables into a data frame
data <- data.frame(Wages, Education, Male, Promotions, IQ, Jan)
# Display the first few rows of the dataset
head(data)
# Estimate the model
true_model <- lm(Wages ~ Education + Male + Promotions + IQ, data = data)
summary(true_model)
#Misspecificed model: omitted variable bias
ovb_model <- lm(Wages ~ Education + Male, data = data)
summary(ovb_model)
#2SLS
first_stage <- lm(Education ~ Jan + Male, data = data)
summary(first_stage)
data$Education_hat <- predict(first_stage)
second_stage <- lm(Wages ~ Education_hat + Male, data = data)
summary(second_stage)
setwd("/Users/mikey/Documents/Academic/UCSD/Macro 210C/Macro210C/HW2/")
# install.packages("haven")
library(haven)
romer <- read_dta("RR_monetary_shock_quarterly.dta")
# Convert the date variable to year-quarter format.
start_date <- as.yearqtr("1960 Q1")  # Adjust the start date if necessary
# Clear all.
rm(list = ls())
# install.packages("quantmod")
library(quantmod)
# Import Federal Funds Rate, Unemployment Rate, and GDP Deflator Inflation rate data from FRED.
R <- getSymbols("FEDFUNDS", src = "FRED", auto.assign = FALSE)
u <- getSymbols("UNRATE", src = "FRED", auto.assign = FALSE)
pi <- getSymbols("GDPDEF", src = "FRED", auto.assign = FALSE)
#Put the data into data frames.
R <- data.frame(date = index(R), R = coredata(R)[, "FEDFUNDS"])
u <- data.frame(date = index(u), u = coredata(u)[, "UNRATE"])
pi <- data.frame(date = index(pi), pi = coredata(pi)[, "GDPDEF"])
### (a) Plot the data. Make sure all graphs are appropriately labelled.
# install.packages("ggplot2")
library(ggplot2)
ggplot(R, aes(x = date, y = R)) + geom_line() + labs(title = "Federal Funds Rate", x = "Date", y = "Percentage Points")
ggplot(u, aes(x = date, y = u)) + geom_line() + labs(title = "Unemployment Rate", x = "Date", y = "Percentage Points")
ggplot(pi, aes(x = date, y = pi)) + geom_line() + labs(title = "GDP Deflator Inflation Rate", x = "Date", y = "Percentage Points")
### (b) Aggregate all series to a quarterly frequency by averaging over months.
R <- aggregate(R$R, list(yearqtr = as.yearqtr(R$date)), mean)
u <- aggregate(u$u, list(yearqtr = as.yearqtr(u$date)), mean)
pi <- aggregate(pi$pi, list(yearqtr = as.yearqtr(pi$date)), mean)
### Estimate a VAR with 4 lags from 1960Q1:2007Q4. The ordering of your variables should be $\pi_t,u_t,R_t$.
# install.packages("vars")
library(vars)
# Merge the data into a single data frame
data <- merge(pi, merge(u, R, by = "yearqtr"), by = "yearqtr")
colnames(data) <- c("yearqtr", "pi", "u", "R")
# Calculate inflation
library(dplyr)
data$pi <- 100 * (data$pi - lag(data$pi, 4)) / lag(data$pi, 4)
# Filter the data to keep observations from 1960Q1 to 2007Q4
start_date <- as.yearqtr("1960 Q1")
end_date <- as.yearqtr("2007 Q4")
data_filtered <- data[data$yearqtr >= start_date & data$yearqtr <= end_date, ]
# Convert the filtered data to a time series object with quarterly frequency
time_series <- ts(data_filtered[, c("pi", "u", "R")], start = c(1960, 1), end = c(2007, 4), frequency = 4)
# Estimate the VAR model with 4 lags
var_model <- VAR(time_series, p = 4, type = "const")
# Print the summary of the VAR model
summary(var_model)
### (c) Briefly, explain why it would make sense to end the sample in 2007Q4?
# That is the onset of the Great Recession, which is probably the most significant economic event since the Great Depression. If we want to test the external validity of our VAR model, we can train it on data from "steady state" (pre-2007) and see how well it performs out of sample. Also, there was some pretty big government expenditure in response to the Great Recession, so if we train the VAR using data from that time, it may be capturing the response of the variables to this government activity (an omitted variable) rather than to natural fluctuations to the rest of the three (included) variables.
### (d) Plot the IRFs from the SVAR with the same ordering. [Optional: add 95% error bands]
irf <- irf(var_model, impulse = c("pi", "u", "R"), response = c("pi", "u", "R"), boot = TRUE, runs = 1000)
# Plot the IRFs
# par(mfrow = c(3, 3))  # Set up a 3x3 plotting space
# Plot each combination of impulse and response
plot(irf, plot.type = "single", bg = "white", lwd = 4 )
# Extract the residuals (shocks) from the VAR model
residuals <- residuals(var_model)
# Adjust the dates to match the residuals (drop the initial lags)
adjusted_dates <- data_filtered$yearqtr[-(1:4)]
# Convert residuals to a data frame for plotting
residuals_df <- data.frame(date = adjusted_dates, residuals)
# Rename the columns to match the variable names
colnames(residuals_df) <- c("date", "pi_shock", "u_shock", "R_shock")
# Plot the time series of the identified monetary shocks
par(mfrow = c(3, 1), mar = c(4, 4, 2, 1))  # Set up a 3x1 plotting space
# Plot each shock time series
plot(residuals_df$date, residuals_df$pi_shock, type = "l", col = "red",
xlab = "", ylab = "Percentage Points", main = "Deviation in pi from VAR Prediction")
plot(residuals_df$date, residuals_df$R_shock, type = "l", col = "blue",
xlab = "", ylab = "Percentage Points", main = "Deviation in Federal Funds Rate (R) from VAR Prediction")
plot(residuals_df$date, residuals_df$u_shock, type = "l", col = "green",
xlab = "", ylab = "Percentage Points", main = "Deviation in Unemployment (u) from VAR Prediction")
### (g) What are the identified monetary shocks in 2001Q3 and 2001Q4? How should one interpret these shocks?
## Romer shocks
### (a) Download the Romer-Romer shocks from my website and merge it with your VAR dataset. Set the values of the Romer shocks to zero before 1969Q1.
setwd("/Users/mikey/Documents/Academic/UCSD/Macro 210C/Macro210C/HW2/")
# install.packages("haven")
library(haven)
romer <- read_dta("RR_monetary_shock_quarterly.dta")
# Convert the date variable to year-quarter format.
start_date <- as.yearqtr("1960 Q1")  # Adjust the start date if necessary
romer$yearqtr <- as.yearqtr(start_date + romer$date/4)
# Merge the datasets.
data_merged <- merge(data_filtered, romer, by = "yearqtr", all.x = TRUE)
# Rename resid_romer to RR
colnames(data_merged)[colnames(data_merged) == "resid_romer"] <- "RR"
# Set the values of the Romer shocks to zero before 1969Q1.
data_merged$RR[data_merged$yearqtr < as.yearqtr("1969 Q1")] <- 0
# # Generate 12 lagged variables for RR using a loop
# for (i in 1:12) {
#   data_merged[[paste0("RR_", i)]] <- lag(data_merged$RR, i)
# }
#
# # Define RR_lags as a vector of RR and all its lags.
# RR_lags <- paste0("RR_", 1:12)
# # Convert RR_lags to a matrix and replace NAs with zeros
# RR_lags_mat <- as.matrix(data_merged[, RR_lags])
# RR_lags_mat[is.na(RR_lags_mat)] <- 0
# Create time series with variables in correct order.
# time_series_romer <- ts(data_merged[, c("pi", "u", "R" )], start = c(1960, 1), end = c(2007, 4), frequency = 4)
# time_series_romer[is.na(time_series_romer)] <- 0
# install.packages("MTS")
library("MTS")
# Create column (as matrix) of RR
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
VARX(time_series, p = 8, xt = RR_column, m = 12)
View(data_merged)
View(RR_column)
help("VARXirf")
var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 12)
var_romer <- VARX(time_series, xt = RR_column, m = 12)
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 1, xt = RR_column, m = 12)
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 1, xt = RR_column, m = 1)
VARXirf(var_romer)
# Create column (as matrix) of RR
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 2, xt = RR_column, m = 1)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 1)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 3)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
source("~/Documents/Academic/UCSD/Macro 210C/Macro210C/HW2/hw2.R", echo=TRUE)
# Define RR_lags as a vector of RR and all its lags.
RR_lags <- c("RR", paste0("RR_", 1:12))
RR_lags_mat <- as.matrix(data_merged[, RR_lags])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0)
# Generate 12 lagged variables for RR using a loop
for (i in 1:12) {
data_merged[[paste0("RR_", i)]] <- lag(data_merged$RR, i)
}
# Define RR_lags as a vector of RR and all its lags.
RR_lags <- c("RR", paste0("RR_", 1:12))
# Convert RR_lags to a matrix and replace NAs with zeros
RR_lags_mat <- as.matrix(data_merged[, RR_lags])
RR_lags_mat[is.na(RR_lags_mat)] <- 0
# Define RR_lags as a vector of RR and all its lags.
RR_lags <- c("RR", paste0("RR_", 1:12))
# Convert RR_lags to a matrix and replace NAs with zeros
RR_lags_mat <- as.matrix(data_merged[, RR_lags])
# Clear all.
rm(list = ls())
# install.packages("quantmod")
library(quantmod)
# Import Federal Funds Rate, Unemployment Rate, and GDP Deflator Inflation rate data from FRED.
R <- getSymbols("FEDFUNDS", src = "FRED", auto.assign = FALSE)
u <- getSymbols("UNRATE", src = "FRED", auto.assign = FALSE)
pi <- getSymbols("GDPDEF", src = "FRED", auto.assign = FALSE)
#Put the data into data frames.
R <- data.frame(date = index(R), R = coredata(R)[, "FEDFUNDS"])
u <- data.frame(date = index(u), u = coredata(u)[, "UNRATE"])
pi <- data.frame(date = index(pi), pi = coredata(pi)[, "GDPDEF"])
### (a) Plot the data. Make sure all graphs are appropriately labelled.
# install.packages("ggplot2")
library(ggplot2)
ggplot(R, aes(x = date, y = R)) + geom_line() + labs(title = "Federal Funds Rate", x = "Date", y = "Percentage Points")
ggplot(u, aes(x = date, y = u)) + geom_line() + labs(title = "Unemployment Rate", x = "Date", y = "Percentage Points")
ggplot(pi, aes(x = date, y = pi)) + geom_line() + labs(title = "GDP Deflator Inflation Rate", x = "Date", y = "Percentage Points")
### (b) Aggregate all series to a quarterly frequency by averaging over months.
R <- aggregate(R$R, list(yearqtr = as.yearqtr(R$date)), mean)
u <- aggregate(u$u, list(yearqtr = as.yearqtr(u$date)), mean)
pi <- aggregate(pi$pi, list(yearqtr = as.yearqtr(pi$date)), mean)
### Estimate a VAR with 4 lags from 1960Q1:2007Q4. The ordering of your variables should be $\pi_t,u_t,R_t$.
# install.packages("vars")
library(vars)
# Merge the data into a single data frame
data <- merge(pi, merge(u, R, by = "yearqtr"), by = "yearqtr")
colnames(data) <- c("yearqtr", "pi", "u", "R")
# Calculate inflation
library(dplyr)
data$pi <- 100 * (data$pi - lag(data$pi, 4)) / lag(data$pi, 4)
# Filter the data to keep observations from 1960Q1 to 2007Q4
start_date <- as.yearqtr("1960 Q1")
end_date <- as.yearqtr("2007 Q4")
data_filtered <- data[data$yearqtr >= start_date & data$yearqtr <= end_date, ]
# Convert the filtered data to a time series object with quarterly frequency
time_series <- ts(data_filtered[, c("pi", "u", "R")], start = c(1960, 1), end = c(2007, 4), frequency = 4)
# Estimate the VAR model with 4 lags
var_model <- VAR(time_series, p = 4, type = "const")
romer <- read_dta("RR_monetary_shock_quarterly.dta")
# Convert the date variable to year-quarter format.
start_date <- as.yearqtr("1960 Q1")  # Adjust the start date if necessary
romer$yearqtr <- as.yearqtr(start_date + romer$date/4)
# Merge the datasets.
data_merged <- merge(data_filtered, romer, by = "yearqtr", all.x = TRUE)
# Rename resid_romer to RR
colnames(data_merged)[colnames(data_merged) == "resid_romer"] <- "RR"
# Set the values of the Romer shocks to zero before 1969Q1.
data_merged$RR[data_merged$yearqtr < as.yearqtr("1969 Q1")] <- 0
# Generate 12 lagged variables for RR using a loop
for (i in 1:12) {
data_merged[[paste0("RR_", i)]] <- lag(data_merged$RR, i)
}
# Define RR_lags as a vector of RR and all its lags.
RR_lags <- c("RR", paste0("RR_", 1:12))
# Convert RR_lags to a matrix and replace NAs with zeros
RR_lags_mat <- as.matrix(data_merged[, RR_lags])
RR_lags_mat[is.na(RR_lags_mat)] <- 0
library("MTS")
# Create column (as matrix) of RR
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0)
VARXirf(var_romer)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0, boot = TRUE)
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0, boot)
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 0)
VARXirf(var_romer)
RR_column <- as.matrix(data_merged[, "RR"])
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_lags_mat, m = 2)
# VAR it up.
# var_romer <- VAR(time_series_romer, p = 8, type = "const", exogen = RR_lags_mat)
# var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
var_romer <- VARX(time_series, p = 8, xt = RR_column, m = 2)
VARXirf(var_romer)
help(detatch)
help(detach)
# Create matrices for ordering.
AA <- diag(1,4)
AA[upper.tri(AA)] <- NA
BB <- diag(4)
# Estimate the SVAR model using the VAR model and identification restrictions.
svar_romer <- SVAR(var_romer, Amat = AA, Bmat = BB, estmethod = "direct", lrtest = FALSE)
# Estimate the VAR model
var_romer <- VAR(time_series_romer, p = 4, type = "const")
var_romer <- VAR(time_series_romer, p = 4, type = "const")
